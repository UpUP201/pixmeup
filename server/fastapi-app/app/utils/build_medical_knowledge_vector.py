import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings  # âœ… ìµœì‹  ë°©ì‹
from langchain_community.vectorstores import FAISS  # âœ… ìµœì‹  ë°©ì‹

load_dotenv()

VECTOR_DIR = "/app/vectorstore/medical_knowledge"
PAPER_DIR = "app/data/papers"

def build_medical_knowledge_index():
    all_docs = []

    for filename in os.listdir(PAPER_DIR):
        if not filename.endswith(".pdf"):
            continue

        path = os.path.join(PAPER_DIR, filename)
        print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {filename}")

        try:
            loader = PyMuPDFLoader(path)
            raw_docs = loader.load()
        except Exception as e:
            print(f"âŒ {filename} ë¡œë”© ì‹¤íŒ¨: {e}")
            continue

        # í…ìŠ¤íŠ¸ ì¡°ê° ë¶„ë¦¬
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = splitter.split_documents(raw_docs)

        # ë©”íƒ€ë°ì´í„°ì— íŒŒì¼ëª… ì¶”ê°€
        for doc in chunks:
            doc.metadata["source"] = filename

        all_docs.extend(chunks)

    # ë²¡í„°í™” ë° ì €ì¥
    if not all_docs:
        print("â— ë…¼ë¬¸ì´ í•˜ë‚˜ë„ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ§  ì´ {len(all_docs)}ê°œ ì¡°ê° ìƒì„± ì™„ë£Œ. ë²¡í„°í™” ì‹œì‘...")

    openai_api_key = os.getenv("OPENAI_API_KEY", "sk-tmp")
    # if not openai_api_key:
    #     raise EnvironmentError("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.from_documents(all_docs, embeddings)
    vectorstore.save_local(VECTOR_DIR)

    print(f"âœ… ë²¡í„° ì €ì¥ ì™„ë£Œ: {VECTOR_DIR}")

if __name__ == "__main__":
    build_medical_knowledge_index()
